"""
ToolExecutor - ReAct å¾ªç¯æ‰§è¡Œå™¨

èŒè´£ï¼š
- æ ¹æ® AgentPlan é©±åŠ¨ ReAct å¾ªç¯
- æµç¨‹ï¼šLLM æ€è€ƒ â†’ é€‰æ‹©å·¥å…· â†’ æ‰§è¡Œ â†’ æ³¨å…¥ observation â†’ ç»§ç»­
- ç®¡ç†æ‰§è¡Œæ­¥éª¤ï¼Œæ”¶é›† AgentStep ç”¨äºå›æ”¾

"""

import json
import logging
import re
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple

from .types import (
    AgentTask, AgentPlan, AgentStep, AgentResult, AgentCitation,
    AgentRunState, ToolResult, ToolTrace,
)

logger = logging.getLogger(__name__)


# ReAct å¾ªç¯çš„ç³»ç»Ÿæç¤ºæ¨¡æ¿
REACT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

## æ‰§è¡Œè®¡åˆ’
{plan}

## å¯ç”¨å·¥å…·
{tools}

## å›å¤æ ¼å¼
æ¯æ¬¡å›å¤å¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼š

1. è°ƒç”¨å·¥å…·æ—¶ï¼š
```json
{{"thought": "æˆ‘çš„æ€è€ƒè¿‡ç¨‹...", "action": "å·¥å…·å", "action_input": {{"å‚æ•°å": "å‚æ•°å€¼"}}}}
```

2. ç»™å‡ºæœ€ç»ˆç­”æ¡ˆæ—¶ï¼š
```json
{{"thought": "æˆ‘çš„æ€è€ƒè¿‡ç¨‹...", "final_answer": "æœ€ç»ˆå›ç­”å†…å®¹"}}
```

æ³¨æ„ï¼šåªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""

# ä¸‹ä¸€æ­¥æç¤ºæ¨¡æ¿
NEXT_STEP_PROMPT = """ç”¨æˆ·é—®é¢˜ï¼š{query}

{observations}

åŸºäºå½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š
1. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œè°ƒç”¨åˆé€‚çš„å·¥å…·
2. å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
3. å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œä½¿ç”¨ final_answer"""


class ToolExecutor:
    """
    ReAct å¾ªç¯æ‰§è¡Œå™¨
    
    è´Ÿè´£æ‰§è¡Œ AgentPlanï¼Œé©±åŠ¨ thought â†’ tool_call â†’ observation å¾ªç¯ã€‚
    
    ReAct å¾ªç¯ï¼š
    1. think() - LLM æ€è€ƒï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
    2. act() - æ‰§è¡Œå·¥å…·è°ƒç”¨
    3. æ”¶é›† observationï¼Œè¿½åŠ åˆ°ä¸Šä¸‹æ–‡
    4. é‡å¤ç›´åˆ°ç»™å‡ºæœ€ç»ˆç­”æ¡ˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
    
    """
    
    # ç‰¹æ®Šå·¥å…·åï¼ˆç»ˆæ­¢æ‰§è¡Œï¼‰
    TERMINAL_TOOL_NAMES = ["terminate", "finish", "final_answer"]
    
    def __init__(self, tool_router=None, max_steps: int = 10, max_observe: int = 2000):
        """
        Args:
            tool_router: ToolRouter å®ä¾‹ï¼Œç”¨äºåˆ†å‘å·¥å…·è°ƒç”¨
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            max_observe: è§‚å¯Ÿç»“æœæœ€å¤§å­—ç¬¦æ•°
        """
        self.tool_router = tool_router
        self.max_steps = max_steps
        self.max_observe = max_observe
        self.state = AgentRunState.IDLE
        
        # å·¥å…·è°ƒç”¨è¿½è¸ª
        self.tool_traces: List[ToolTrace] = []
    
    async def execute(
        self,
        task: AgentTask,
        plan: AgentPlan,
        context: Optional[dict] = None,
        system_prompt: str = "",
        available_tools: Optional[List[str]] = None,
    ) -> AgentResult:
        """
        æ‰§è¡Œ ReAct å¾ªç¯
        
        æµç¨‹ï¼š
        1. åˆå§‹åŒ–çŠ¶æ€ä¸º RUNNING
        2. å¾ªç¯æ‰§è¡Œ step() = think() + act()
        3. æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°
        4. è¿”å›ç»“æœï¼ˆåŒ…å« tool_tracesï¼‰
        
        Args:
            task: AgentTask è¾“å…¥
            plan: TaskPlanner ç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆmessages ç­‰ï¼‰
            system_prompt: åŸºç¡€ system prompt
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            
        Returns:
            AgentResult åŒ…å«æœ€ç»ˆç­”æ¡ˆã€æ‰§è¡Œæ­¥éª¤å’Œå·¥å…·è¿½è¸ª
        """
        steps: List[AgentStep] = []
        citations: List[AgentCitation] = []
        observations: List[str] = []
        self.tool_traces = []  # é‡ç½®å·¥å…·è¿½è¸ª
        
        # çŠ¶æ€è½¬æ¢
        self.state = AgentRunState.RUNNING
        
        try:
            # å¦‚æœæ˜¯ç®€å•è®¡åˆ’ï¼Œç›´æ¥è°ƒç”¨ LLM å›ç­”
            if len(plan.steps) == 1 and plan.steps[0] == "ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜":
                logger.info("Simple plan, using direct LLM response")
                answer = await self._direct_llm_response(
                    task.query, 
                    system_prompt, 
                    context
                )
                self.state = AgentRunState.FINISHED
                return AgentResult(
                    answer=answer,
                    citations=citations,
                    steps=[AgentStep(step_idx=0, thought="ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜")],
                    tool_traces=self.tool_traces,
                )
            
            # è·å–å·¥å…· schema
            tool_schemas = []
            if self.tool_router and available_tools:
                tool_schemas = self.tool_router.list_tool_schemas(available_tools)
            
            # ReAct ä¸»å¾ªç¯
            for step_idx in range(self.max_steps):
                # æ£€æŸ¥çŠ¶æ€
                if self.state == AgentRunState.FINISHED:
                    break
                
                logger.info(f"ğŸ”„ Executing step {step_idx + 1}/{self.max_steps}")
                
                # Step = Think + Act
                step, should_continue = await self._step(
                    step_idx=step_idx,
                    task=task,
                    plan=plan,
                    tool_schemas=tool_schemas,
                    observations=observations,
                    system_prompt=system_prompt,
                    context=context,
                )
                
                steps.append(step)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç­”æ¡ˆ
                if step.observation and "final_answer:" in step.observation.lower():
                    # æå–æœ€ç»ˆç­”æ¡ˆ
                    answer = step.observation.split("final_answer:", 1)[-1].strip()
                    if answer.startswith('"') and answer.endswith('"'):
                        answer = answer[1:-1]
                    self.state = AgentRunState.FINISHED
                    logger.info(f"ğŸ Got final answer at step {step_idx + 1}")
                    return AgentResult(
                        answer=answer,
                        citations=citations,
                        steps=steps,
                        tool_traces=self.tool_traces,
                    )
                
                if not should_continue:
                    break
            
            # è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œå¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆ
            if self.state != AgentRunState.FINISHED:
                logger.warning(f"âš ï¸ Reached max steps ({self.max_steps}), forcing final answer")
                
                final_answer = await self._generate_final_answer(
                    task.query,
                    observations,
                    system_prompt,
                )
                
                steps.append(AgentStep(
                    step_idx=len(steps),
                    thought="è¾¾åˆ°æœ€å¤§æ‰§è¡Œæ­¥æ•°ï¼Œå¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆ",
                ))
                
                self.state = AgentRunState.FINISHED
                return AgentResult(
                    answer=final_answer,
                    citations=citations,
                    steps=steps,
                    tool_traces=self.tool_traces,
                )
            
            # æ­£å¸¸å®Œæˆ
            return AgentResult(
                answer="ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
                citations=citations,
                steps=steps,
                tool_traces=self.tool_traces,
            )
            
        except Exception as e:
            self.state = AgentRunState.ERROR
            logger.error(f"ğŸš¨ Execution error: {e}")
            return AgentResult(
                answer=f"æ‰§è¡Œå‡ºé”™: {str(e)}",
                citations=[],
                steps=steps,
                tool_traces=self.tool_traces,
            )
    
    async def _step(
        self,
        step_idx: int,
        task: AgentTask,
        plan: AgentPlan,
        tool_schemas: List[Dict],
        observations: List[str],
        system_prompt: str,
        context: Optional[dict],
    ) -> Tuple[AgentStep, bool]:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤ (think + act)
        
        Returns:
            (AgentStep, should_continue)
        """
        # Think: è°ƒç”¨ LLM è·å–å†³ç­–
        messages = self._build_messages(
            task.query,
            plan,
            tool_schemas,
            observations,
            system_prompt,
            context,
        )
        
        response = await self._call_llm(messages)
        
        # è§£æå“åº”
        parsed = self._parse_response(response)
        
        thought = parsed.get("thought", "")
        action = parsed.get("action")
        action_input = parsed.get("action_input", {})
        final_answer = parsed.get("final_answer")
        
        # è®°å½•æ­¥éª¤
        step = AgentStep(
            step_idx=step_idx,
            thought=thought,
            tool_name=action,
            tool_args=action_input if action else None,
        )
        
        logger.info(f"âœ¨ Thought: {thought[:100]}...")
        
        # å¦‚æœæœ‰æœ€ç»ˆç­”æ¡ˆ
        if final_answer:
            step.observation = f"final_answer: {final_answer}"
            logger.info(f"ğŸ¯ Final answer received")
            self.state = AgentRunState.FINISHED
            return step, False
        
        # Act: æ‰§è¡Œå·¥å…·è°ƒç”¨
        if action and self.tool_router:
            observation = await self._execute_tool(action, action_input)
            
            # æˆªæ–­è§‚å¯Ÿç»“æœ
            if len(observation) > self.max_observe:
                observation = observation[:self.max_observe] + "... (truncated)"
            
            step.observation = observation
            observations.append(f"[Step {step_idx + 1}] {observation}")
            
            logger.info(f"ğŸ¯ Tool '{action}' result: {observation[:100]}...")
            
            # æ£€æŸ¥ç»ˆæ­¢å·¥å…·
            if self._is_terminal_tool(action):
                logger.info(f"ğŸ Terminal tool '{action}' completed the task!")
                self.state = AgentRunState.FINISHED
                return step, False
            
            return step, True
        
        # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¹Ÿæ²¡æœ‰æœ€ç»ˆç­”æ¡ˆ
        step.observation = f"æ— æ³•è§£æå“åº”: {response[:200]}"
        step.error = "è§£æå¤±è´¥"
        observations.append(f"[Step {step_idx + 1}] è§£æå¤±è´¥")
        
        return step, True
    
    def _is_terminal_tool(self, name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç»ˆæ­¢å·¥å…·"""
        return name.lower() in [n.lower() for n in self.TERMINAL_TOOL_NAMES]
    
    async def _execute_tool(self, name: str, args: Dict[str, Any]) -> str:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨
        
        åŒ…å«é”™è¯¯å¤„ç†å’Œè¿½è¸ªè®°å½•ï¼ˆToolTraceï¼‰
        """
        if not name:
            return "Error: Invalid tool name"
        
        logger.info(f"ğŸ”§ Activating tool: '{name}'...")
        started_at = datetime.now()
        
        try:
            result = await self.tool_router.execute(name, args)
            finished_at = datetime.now()
            
            # æ„å»º ToolResult
            tool_result = ToolResult(
                success=True,
                output=result,
                summary=str(result)[:500] if result else None,
            )
            
            # è®°å½• ToolTrace
            trace = ToolTrace(
                tool_name=name,
                tool_args=args,
                result=tool_result,
                started_at=started_at,
                finished_at=finished_at,
            )
            self.tool_traces.append(trace)
            
            # æ ¼å¼åŒ–è¾“å‡º
            observation = (
                f"Observed output of tool `{name}`:\n{str(result)}"
                if result
                else f"Tool `{name}` completed with no output"
            )
            return observation
            
        except Exception as e:
            finished_at = datetime.now()
            error_msg = str(e)
            
            # æ„å»ºå¤±è´¥çš„ ToolResult
            tool_result = ToolResult(
                success=False,
                error=error_msg,
            )
            
            # è®°å½• ToolTrace
            trace = ToolTrace(
                tool_name=name,
                tool_args=args,
                result=tool_result,
                started_at=started_at,
                finished_at=finished_at,
            )
            self.tool_traces.append(trace)
            
            logger.error(f"Tool '{name}' error: {error_msg}")
            return f"âš ï¸ Tool '{name}' error: {error_msg}"
    
    def _build_messages(
        self,
        query: str,
        plan: AgentPlan,
        tool_schemas: List[Dict],
        observations: List[str],
        base_system_prompt: str,
        context: Optional[dict],
    ) -> List[Dict[str, str]]:
        """æ„å»º ReAct å¾ªç¯çš„æ¶ˆæ¯"""
        # æ ¼å¼åŒ–è®¡åˆ’
        plan_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(plan.steps)])
        
        # æ ¼å¼åŒ–å·¥å…·
        tools_text = self._format_tools_for_prompt(tool_schemas)
        
        # ç³»ç»Ÿæç¤º
        system_content = base_system_prompt + "\n\n" + REACT_SYSTEM_PROMPT.format(
            plan=plan_text,
            tools=tools_text,
        )
        
        messages = [{"role": "system", "content": system_content}]
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if context and context.get("messages"):
            for msg in context["messages"]:
                if msg.get("role") != "system":
                    messages.append(msg)
        
        # ç”¨æˆ·æ¶ˆæ¯ + è§‚å¯Ÿç»“æœ
        obs_text = "\n".join(observations) if observations else "ï¼ˆå°šæ— è§‚å¯Ÿç»“æœï¼‰"
        user_content = NEXT_STEP_PROMPT.format(
            query=query,
            observations=obs_text,
        )
        messages.append({"role": "user", "content": user_content})
        
        return messages
    
    def _format_tools_for_prompt(self, tool_schemas: List[Dict]) -> str:
        """æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨ä¾› prompt ä½¿ç”¨"""
        if not tool_schemas:
            return "æ— å¯ç”¨å·¥å…·"
        
        lines = []
        for schema in tool_schemas:
            func = schema.get("function", {})
            name = func.get("name", "unknown")
            desc = func.get("description", "")
            params = func.get("parameters", {}).get("properties", {})
            
            param_str = ", ".join(params.keys()) if params else "æ— å‚æ•°"
            lines.append(f"- {name}: {desc} (å‚æ•°: {param_str})")
        
        return "\n".join(lines)
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM å“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            return json.loads(response)
        except json.JSONDecodeError:
            pass
        
        # å°è¯•æå– JSON å—
        patterns = [
            r'```json\s*(.*?)\s*```',
            r'```\s*(.*?)\s*```',
            r'\{.*\}',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                try:
                    json_str = match.group(1) if '```' in pattern else match.group()
                    return json.loads(json_str)
                except (json.JSONDecodeError, IndexError):
                    continue
        
        # è§£æå¤±è´¥ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
        result = {"thought": response[:200]}
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ€ç»ˆç­”æ¡ˆçš„å…³é”®è¯
        if "final_answer" in response.lower() or "æœ€ç»ˆç­”æ¡ˆ" in response:
            result["final_answer"] = response
        
        return result
    
    async def _call_llm(self, messages: List[Dict[str, str]]) -> str:
        """è°ƒç”¨ LLM"""
        try:
            from services.ai.llm import LLMManager
            
            llm = LLMManager()
            return llm.chat(messages)
            
        except Exception as e:
            logger.error(f"ğŸš¨ LLM call failed: {e}")
            return f'{{"thought": "LLM è°ƒç”¨å¤±è´¥", "final_answer": "æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"}}'
    
    async def _direct_llm_response(
        self, 
        query: str, 
        system_prompt: str,
        context: Optional[dict],
    ) -> str:
        """ç›´æ¥ LLM å“åº”ï¼ˆä¸èµ° ReActï¼‰"""
        messages = [{"role": "system", "content": system_prompt}]
        
        if context and context.get("messages"):
            for msg in context["messages"]:
                if msg.get("role") != "system":
                    messages.append(msg)
        
        messages.append({"role": "user", "content": query})
        
        return await self._call_llm(messages)
    
    async def _generate_final_answer(
        self,
        query: str,
        observations: List[str],
        system_prompt: str,
    ) -> str:
        """åŸºäºè§‚å¯Ÿç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        obs_text = "\n".join(observations) if observations else "æ— è§‚å¯Ÿç»“æœ"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"""ç”¨æˆ·é—®é¢˜ï¼š{query}

æ‰§è¡Œè¿‡ç¨‹ä¸­çš„è§‚å¯Ÿç»“æœï¼š
{obs_text}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºæœ€ç»ˆçš„å›ç­”ã€‚"""},
        ]
        
        return await self._call_llm(messages)
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info(f"ğŸ§¹ Cleaning up ToolExecutor resources...")
        self.state = AgentRunState.IDLE
        self.tool_traces = []
