"""
SearchAgentService - æ·±åº¦ Web æœç´¢æœåŠ¡

èŒè´£ï¼š
- å¤šå­æŸ¥è¯¢ç”Ÿæˆ + å¤šè·¯æœç´¢ + èšåˆå›ç­”
- ä½œä¸º deep_web_research Tool çš„åç«¯å®ç°

å†…éƒ¨æµç¨‹ï¼š
1. QueryInterpreter: ç†è§£/é‡å†™é—®é¢˜
2. QueryDecomposer: ç”Ÿæˆå­æŸ¥è¯¢
3. SearchExecutor: å¤šè·¯æœç´¢
4. PageFetcher: æŠ“å–æ­£æ–‡ï¼ˆå¯é€‰ï¼‰
5. EvidenceAggregator: å»é‡+æ’åº+èšåˆ
6. AnswerSynthesizer: ç»¼åˆå›ç­”
"""

import json
import logging
import re
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any

from .http_client import get_search_provider, SearchProvider, WebSearchResult

logger = logging.getLogger(__name__)


@dataclass
class SearchSource:
    """æœç´¢æ¥æº"""
    url: str
    title: Optional[str] = None
    snippet: Optional[str] = None
    content: Optional[str] = None  # è§£æåçš„æ­£æ–‡ï¼ˆå¯é€‰ï¼‰
    score: Optional[float] = None  # ç›¸å…³åº¦/æ’åºç”¨


@dataclass
class SearchAgentResult:
    """æœç´¢ Agent ç»“æœ"""
    query: str                      # åŸå§‹æˆ–é‡å†™åçš„æŸ¥è¯¢
    sub_queries: List[str]          # æ‹†è§£å‡ºçš„å­æŸ¥è¯¢åˆ—è¡¨
    sources: List[SearchSource]     # æ•´ç†åçš„å¤šæºè¯æ®
    answer: str                     # åŸºäºè¯æ®çš„ç»¼åˆå›ç­”


# ============================================================
# Prompt keys (ä» ControlPlane è·å–)
# ============================================================

QUERY_DECOMPOSE_PROMPT_KEY = "alice.search.query_decompose"
ANSWER_SYNTHESIS_PROMPT_KEY = "alice.search.answer_synthesis"


class SearchAgentService:
    """
    æ·±åº¦ Web æœç´¢æœåŠ¡
    
    å®ç°å¤šæ­¥æœç´¢æµç¨‹ï¼šæŸ¥è¯¢ç†è§£ â†’ å­æŸ¥è¯¢æ‹†è§£ â†’ å¤šè·¯æœç´¢ â†’ èšåˆ â†’ å›ç­”
    """
    
    def __init__(self, search_provider: Optional[SearchProvider] = None):
        """
        Args:
            search_provider: æœç´¢æä¾›è€…ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡é…ç½®è·å–
        """
        self.search_provider = search_provider or get_search_provider()
    
    async def run(
        self, 
        query: str, 
        user_context: Optional[Dict[str, Any]] = None,
        max_steps: int = 4
    ) -> SearchAgentResult:
        """
        æ‰§è¡Œæ·±åº¦ Web æœç´¢
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆå·²çŸ¥ä¿¡æ¯ç­‰ï¼‰
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°ï¼ˆé¢„ç•™ï¼Œå½“å‰æœªä½¿ç”¨ï¼‰
            
        Returns:
            SearchAgentResult åŒ…å«ç»¼åˆç­”æ¡ˆå’Œæ¥æº
        """
        logger.info(f"ğŸ” Starting deep web search for: {query[:50]}...")
        
        # Step 1: è§„èŒƒåŒ–/ç†è§£é—®é¢˜
        normalized_query = await self._interpret_query(query, user_context)
        
        # Step 2: ç”Ÿæˆå­æŸ¥è¯¢
        sub_queries = await self._decompose_query(normalized_query)
        logger.info(f"ğŸ“‹ Generated {len(sub_queries)} sub-queries: {sub_queries}")
        
        # Step 3: å¯¹æ¯ä¸ªå­æŸ¥è¯¢æ‰§è¡Œæœç´¢
        all_sources: List[SearchSource] = []
        for sub_q in sub_queries:
            sub_sources = await self._search_single_query(sub_q)
            all_sources.extend(sub_sources)
        logger.info(f"ğŸŒ Fetched {len(all_sources)} total sources")
        
        # Step 4: å¯é€‰ - æŠ“å–æ­£æ–‡ï¼ˆå½“å‰è·³è¿‡ï¼Œåªç”¨ snippetï¼‰
        # enriched_sources = []
        # for src in all_sources:
        #     enriched_sources.append(await self._fetch_and_analyze(src))
        
        # Step 5: èšåˆå»é‡
        final_sources = self._aggregate_sources(all_sources)
        logger.info(f"ğŸ“Š Aggregated to {len(final_sources)} unique sources")
        
        # Step 6: ç»¼åˆå›ç­”
        answer = await self._synthesize_answer(normalized_query, final_sources)
        
        return SearchAgentResult(
            query=normalized_query,
            sub_queries=sub_queries,
            sources=final_sources,
            answer=answer,
        )
    
    async def _interpret_query(
        self, 
        query: str, 
        user_context: Optional[Dict[str, Any]]
    ) -> str:
        """
        è§„èŒƒåŒ–/å¢å¼ºç”¨æˆ·é—®é¢˜
        
        ç»“åˆ user_context å¯¹ query åšæœ€å°ç¨‹åº¦é‡å†™ï¼š
        - è¡¥å…¨çœç•¥ä¿¡æ¯ï¼ˆå¦‚æ—¶é—´èŒƒå›´ï¼‰
        - å»é™¤æ— å…³è¯­æ°”è¯
        """
        # ç®€å•æ¸…ç†
        query = query.strip()
        
        # ç§»é™¤å¸¸è§è¯­æ°”è¯
        for phrase in ["è¯·é—®", "å¸®æˆ‘", "æˆ‘æƒ³çŸ¥é“", "èƒ½å‘Šè¯‰æˆ‘"]:
            query = query.replace(phrase, "")
        
        query = query.strip()
        
        # å¦‚æœæœ‰æ—¶é—´ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è¡¥å……
        if user_context and user_context.get("current_year"):
            # æ£€æµ‹æ˜¯å¦éœ€è¦æ—¶é—´é™å®š
            time_keywords = ["æœ€æ–°", "è¿‘æœŸ", "æœ€è¿‘", "ç°åœ¨", "å½“å‰"]
            if any(kw in query for kw in time_keywords):
                query = f"{query} {user_context['current_year']}"
        
        return query
    
    async def _decompose_query(
        self, 
        query: str, 
        max_sub_queries: int = 3
    ) -> List[str]:
        """
        å°†å¤æ‚é—®é¢˜æ‹†è§£æˆ 1~N ä¸ªå­æŸ¥è¯¢
        """
        # ç®€å•è§„åˆ™ï¼šæ£€æµ‹æ˜¯å¦éœ€è¦æ‹†è§£
        decompose_keywords = ["å¯¹æ¯”", "åŒºåˆ«", "å’Œ", "ä¸", "å†å²", "å‘å±•", "åŸå› ", "å½±å“"]
        
        need_decompose = any(kw in query for kw in decompose_keywords) and len(query) > 20
        
        if not need_decompose:
            return [query]
        
        # ä½¿ç”¨ LLM æ‹†è§£
        try:
            from alice.control_plane import get_control_plane
            
            cp = get_control_plane()
            llm = cp.create_llm_for_task_sync("chat")
            prompt_template = cp.get_prompt_sync(QUERY_DECOMPOSE_PROMPT_KEY)
            prompt = prompt_template.format(query=query)
            response = llm.chat([{"role": "user", "content": prompt}])
            
            # è§£æ JSON
            match = re.search(r'\[.*\]', response, re.DOTALL)
            if match:
                sub_queries = json.loads(match.group())
                if isinstance(sub_queries, list) and len(sub_queries) > 0:
                    return sub_queries[:max_sub_queries]
        except Exception as e:
            logger.warning(f"Query decomposition failed: {e}")
        
        # é™çº§ï¼šè¿”å›åŸæŸ¥è¯¢
        return [query]
    
    async def _search_single_query(
        self, 
        query: str, 
        top_k: int = 5
    ) -> List[SearchSource]:
        """
        å¯¹å•ä¸ªå­æŸ¥è¯¢æ‰§è¡Œæœç´¢
        """
        results = await self.search_provider.search(query, top_k=top_k)
        
        return [
            SearchSource(
                url=r.url,
                title=r.title,
                snippet=r.snippet,
                score=r.score,
            )
            for r in results
        ]
    
    async def _fetch_and_analyze(self, source: SearchSource) -> SearchSource:
        """
        æŠ“å–é¡µé¢æ­£æ–‡å¹¶åˆ†æï¼ˆå¯é€‰å¢å¼ºï¼‰
        
        å½“å‰è¿”å›åŸ sourceï¼Œåç»­å¯æ¥å…¥ readability / trafilatura
        """
        # TODO: å®ç°æ­£æ–‡æŠ“å–
        # try:
        #     async with httpx.AsyncClient() as client:
        #         resp = await client.get(source.url, timeout=10)
        #         # ä½¿ç”¨ readability æˆ– trafilatura æå–æ­£æ–‡
        #         source.content = extract_content(resp.text)
        # except:
        #     pass
        return source
    
    def _aggregate_sources(
        self, 
        all_sources: List[SearchSource], 
        max_sources: int = 10
    ) -> List[SearchSource]:
        """
        åˆå¹¶å¤šä¸ªå­æŸ¥è¯¢çš„ç»“æœï¼šå»é‡ + æ’åº + æˆªæ–­
        """
        # æŒ‰ URL å»é‡
        seen_urls = set()
        unique_sources = []
        
        for src in all_sources:
            if src.url not in seen_urls:
                seen_urls.add(src.url)
                unique_sources.append(src)
        
        # æŒ‰ score æ’åºï¼ˆå¦‚æœæœ‰ï¼‰
        unique_sources.sort(
            key=lambda s: s.score if s.score is not None else 0,
            reverse=True
        )
        
        return unique_sources[:max_sources]
    
    async def _synthesize_answer(
        self,
        query: str,
        sources: List[SearchSource],
    ) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆç»¼åˆå›ç­”
        """
        if not sources:
            return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚"
        
        # æ ¼å¼åŒ–æ¥æº
        sources_text = ""
        for i, src in enumerate(sources, 1):
            sources_text += f"[[{i}]] {src.title or 'Untitled'}\n"
            sources_text += f"    URL: {src.url}\n"
            sources_text += f"    æ‘˜è¦: {src.snippet or src.content or 'N/A'}\n\n"
        
        try:
            from alice.control_plane import get_control_plane
            
            cp = get_control_plane()
            llm = cp.create_llm_for_task_sync("chat")
            prompt_template = cp.get_prompt_sync(ANSWER_SYNTHESIS_PROMPT_KEY)
            prompt = prompt_template.format(
                query=query,
                sources=sources_text,
            )
            answer = llm.chat([{"role": "user", "content": prompt}])
            return answer
            
        except Exception as e:
            logger.error(f"Answer synthesis failed: {e}")
            # é™çº§ï¼šè¿”å›æ¥æºæ‘˜è¦
            return f"æœç´¢æ‰¾åˆ° {len(sources)} ä¸ªç›¸å…³ç»“æœï¼Œä½†æ— æ³•ç”Ÿæˆç»¼åˆå›ç­”ã€‚\n\n{sources_text}"
