{
  "text": "今天我们来讲多智能体评估，怎么知道它表现的好不好？在传统的软件开发中，测试是僵化的，输入X函数必须严格按照预设制走ABC执行，最后输出外任何步骤的偏差都会被判定为fail，但多智能体系统不是这样工作的，即使起点完全相同，智能体每次可能会走完全不同的路径。A正态可能先搜了google A正B可能先查了学术库，他们的路径完全不同，但可能都得出了正确的结果，这就带来了。一个评估难题。如果我们只看结果可能漏掉过程中的风险，比如他是蒙对的，或者用了不可信的手段。如果我们死板的检查步骤，又会误杀那些创新的路径，但依然有效的聪明agent，所以anthropic提出了一种辩证的评估哲学。我们需要一种灵活的方法。既要判断智能体是否取得了正确的结果，又要评估它的执行过程是否合理，请注意是合理，而不是预设，只要过程符合逻辑。哪怕他走了一条我们没想到的路，也是好 agent。😊此外，anthrop给我们提供了几个评估的策略。策略一，从小样本立刻开始，很多开发团队都有一个误区，认为评估是一个大工程，必须攒够几百个测试用例才开始。但anthrop给出了截然相反的建议，立刻开始，哪怕样本很小。Yeah。为什么？因为在agent开发的早期阶段，遍地都是低垂的果实。在这个阶段，系统的基准能力往往很低。这时候你哪怕只是微调了一句prot，或者修复了一个简单的工具描述，成功率可能就会瞬间从30%飙升到80%。面对这种巨大的效应规模。你根本不需要大样本来验证。原文明确提到do团队最初只用了大约20个真实用例，就构建了第一版测试集，这就足够了。所以给所有开发者的第一条建议是。不要等，现在就找20个真实的例子跑起来。策略2LLM裁判的规模化。当你度过了早期阶段，需要扩大测试规模时，你会面临第二个难题，如何自动化评分多智能体系统的输出通常是自由格式的文本，很难用代码断言。这时候让LLM当裁判就成了自然的选择。但这并不是随便打个分anthropic分享了他们精心设计的五维评分量规。一、事实准确性，说的对。不对，2、引用准确性引用的链接真的支持该观点吗？安、完整性是否覆盖了用户的所有要求。4、信源质量是引用了一手资料还是地摊文学。5、工具效率有没有浪费token在工程实现上结论非常务实。单次调用最优，只需一次LLM调用，让他输出0比1的分数和通过失败结论，这不仅成本低，而且与人类判断的一致性极高。这让规模化评估成为了可能。策略3，人类评估捕捉盲区。有了自动化裁判，我们还需要人类吗？anthropic的答案是必须需要。因为自动化评估只能测出下限，而人类评估能发现系统性偏见。原文分享了一个极具警示意义的真实案例。SEO内容陷阱，在早期测试中，LLM裁判觉得系统表现不错，但人类测试者发现，Aent似乎特别喜欢引用那些标题党SEO文，而忽略了权威。😡但排名不高的学术PDF因为LLM训练数据里有很多SEO文章，他天生觉得这种文章看起来很相关，这就是自动化评估的盲区。只有人类专家能一眼看穿这个信源是垃圾。发现这个问题后，工程团队才在prot中加入了关于信源质量的强制规则。策略4，注意多智能体的涌现行为。什么是涌现。简单来说就是系统表现出了你从未显示编程过的行为。多智能体系统是一个非线性的复杂性。😊统你可能只是微调了主智能体的一点点prot，比如改变了他说话的语气。结果这种微小的变化在系统中传递放大，可能会不可预测的，彻底改变子智能体的行为方式，也许子智能体突然变得不敢用工具了，也许他们开始过度沟通了，这就是系统工程中的蝴蝶效应。面对这种不可预测性，评估的重点不再是单个agent的对错，而是理解整个系统的协作模式。原文给出了一个极具深度的建议。最好的pro。不仅仅是严格的指令，而应该是协作框架。这意味着我们在设计prompt时不能只关注单点任务，而要定义好三件事。一、分工明确谁该干什么。2、解题方法论，定义遇到问题时的思考路径。3、努力预算规定什么时候该坚持什么时候该放弃。要想驾驭这种涌现，你不能靠运气，而是要依靠精心的工具设计，稳固的启发式规则，全面的可观测性。。及紧密的反馈闭环，只有做好了这些，你才能把混乱的涌现变成智能的涌现。总结一下，评估多智能体系统本质上是在评估一个过程。一、起步阶段不要等待大数据，用20个真实用例抓住30%到80%的低垂果实。2、扩展阶段，利用LLM裁判，建立五维评分量规，实现规模化验证。3、兜底阶段，让人类介入捕捉那些自动化裁判。看不懂。😡的性源偏见和幻觉。4、演进阶段，时刻警惕系统的涌现行为，把评估的重点放在协作模式的合理性上。好了，本期就到这里，我们下期详细讲解多agt生产可靠性及工程挑战。如果您觉得对你有用，欢迎一键三连。",
  "language": "zh",
  "duration": 0.0,
  "segments": []
}